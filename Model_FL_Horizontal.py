{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f0F1zJxI53HF"},"outputs":[],"source":["!pip install --quiet --upgrade tensorflow-federated\n","!pip install git+https://github.com/AutoViML/featurewiz.git\n","# !pip install featurewiz --ignore-installed --no-deps\n","# !pip install xlrd --ignore-installed --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2234,"status":"ok","timestamp":1687286579053,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"GoVjLHT7nb5Y","outputId":"efb85205-ad67-43f5-c6a0-9febaed56b59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow_federated as tff\n","import featurewiz as FW\n","import tensorflow as tf\n","from google.colab import drive\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif, RFECV, RFE\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.metrics import accuracy_score, explained_variance_score\n","from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC, LinearSVC\n","import pickle\n","import inspect\n","from tensorflow import keras\n","import time\n","import joblib\n","import attr\n","import matplotlib.pyplot as plt\n","import collections\n","import sys\n","import nest_asyncio\n","import math\n","\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omT-AsBDnxrg"},"outputs":[],"source":["def selectFW(dataset):\n","\n","  out1, out2 = FW.featurewiz(dataname=dataset, target='ataque', corr_limit=0.40, verbose=0, sep=',',\n","      header=0, test_data='',feature_engg='', category_encoders='',\n","      dask_xgboost_flag=False, nrows=None)\n","\n","  return out1, out2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SL0wGRFUoUMf"},"outputs":[],"source":["def metFilter(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  #f_regression\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('selector', SelectKBest(f_classif, k=10)), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['selector'].get_support()]\n","  df_selected = pd.DataFrame(pipe.named_steps['selector'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","  #WRAPPER\n","def metWrapper(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  estimator = LogisticRegression()\n","\n","  # Criando um seletor de recursos com RFE\n","  selector = RFE(estimator, n_features_to_select=10, step=1)\n","\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('rfe', selector), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  # Executando a seleção de recursos\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['rfe'].get_support()]\n","\n","  df_selected = pd.DataFrame(pipe.named_steps['rfe'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","def metIntrinseco(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  rfc = RandomForestClassifier(n_estimators=100, random_state=42, max_features=10)\n","\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('rfc', SelectFromModel(rfc, threshold=-np.inf, max_features=10)), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['rfc'].get_support()]\n","  df_selected = pd.DataFrame(pipe.named_steps['rfc'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","def metResult(arrayFilter, arrayWrapper,arrayIntrinseco):\n","  arrayResult = []\n","  for f in range(len(arrayFilter)):\n","    for w in range(len(arrayWrapper)):\n","      for i in range(len(arrayIntrinseco)):\n","        if ((arrayFilter[f] == arrayWrapper[w]) and (arrayFilter[f] == arrayIntrinseco[i])):\n","          arrayResult.append(arrayFilter[i])\n","  return arrayResult\n","\n","def selecionarAtributos(dataset):\n","    features, dataset = selectFW(dataset)\n","\n","    arrayFilter = metFilter(dataset)\n","    arrayWrapper = metWrapper(dataset)\n","    arrayIntrinseco = metIntrinseco(dataset)\n","\n","    arrayResult = metResult(arrayFilter,arrayWrapper,arrayIntrinseco)\n","\n","    arrayMetodos = [arrayResult, arrayFilter, arrayWrapper, arrayIntrinseco]\n","\n","    arrayMetodos = [arr for arr in arrayMetodos if len(arr) > 0]\n","\n","    return arrayMetodos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuBf5QQqA_1Q"},"outputs":[],"source":["#mede a acurácia de todos os atributos e retornando o com maior acurácia\n","def medirAcuracia(dataFrame, arrayRecursos):\n","  maiorAcuracia = 0\n","  melhoresRecursos = []\n","  for recursos in arrayRecursos:\n","\n","    array = np.append(recursos, 'ataque')\n","    newDataFrame = dataFrame.loc[:, array]\n","\n","    X = newDataFrame.drop('ataque', axis=1)\n","    y = newDataFrame['ataque']\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","    #treinamento\n","    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    rf.fit(X_train, y_train)\n","\n","    y_pred = rf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    if accuracy > maiorAcuracia:\n","      maiorAcuracia = accuracy\n","      melhoresRecursos = recursos\n","\n","  return maiorAcuracia, melhoresRecursos\n","\n","def menorArray(array):\n","  min_length = float('inf')  # inicializa o valor mínimo como infinito\n","  min_array = None  # inicializa o sub-array correspondente como vazio\n","\n","  for sub_array in array:\n","      if len(sub_array) < min_length:\n","          min_length = len(sub_array)\n","          min_array = sub_array\n","  return min_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jA1mO6Shjaam"},"outputs":[],"source":["def lerNode(datasetName):\n","  node1 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node1.csv')\n","  node2 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node2.csv')\n","  node3 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node3.csv')\n","  node4 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node4.csv')\n","\n","  nodesArray = [node1,node2,node3,node4]\n","\n","  return nodesArray"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101637,"status":"ok","timestamp":1687286680685,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"OiJ3ykpPyWDz","outputId":"f2325939-9165-4213-b863-827006dafc20"},"outputs":[{"name":"stdout","output_type":"stream","text":["featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (8094, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (94) highly correlated variables:\n","    Following (21) vars selected: ['H_L001_mean', 'MI_dir_L001_mean', 'MI_dir_L001_weight', 'H_L001_weight', 'H_L01_mean', 'HH_L001_weight', 'HpHp_L001_weight', 'H_L001_variance', 'HH_jit_L01_weight', 'H_L5_mean', 'H_L01_variance', 'HH_L001_covariance', 'HpHp_L1_weight', 'MI_dir_L5_variance', 'HH_jit_L3_weight', 'HH_L5_weight', 'MI_dir_L3_variance', 'H_L3_variance', 'HH_L3_pcc', 'HH_jit_L001_variance', 'HH_jit_L3_variance']\n","Completed SULOV. 21 features selected\n","Time taken for SULOV method = 14 seconds\n","Finally 21 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L001_mean', 'HH_L001_weight', 'H_L001_variance', 'MI_dir_L001_weight', 'HpHp_L001_weight', 'HH_jit_L001_variance', 'H_L5_mean', 'HH_L001_covariance', 'H_L01_variance', 'H_L01_mean']\n","            Time taken for regular XGBoost feature selection = 4 seconds\n","        Selected: ['H_L01_mean', 'H_L001_variance', 'HH_jit_L001_variance', 'HpHp_L001_weight', 'HH_L001_weight', 'H_L5_mean', 'MI_dir_L5_variance', 'MI_dir_L3_variance', 'HH_jit_L01_weight', 'H_L01_variance']\n","            Time taken for regular XGBoost feature selection = 5 seconds\n","        Selected: ['H_L01_variance', 'H_L5_mean', 'MI_dir_L5_variance', 'HH_jit_L001_variance', 'MI_dir_L3_variance', 'HH_L001_covariance', 'HH_jit_L3_weight', 'HH_jit_L01_weight', 'HpHp_L1_weight']\n","            Time taken for regular XGBoost feature selection = 3 seconds\n","        Selected: ['HH_jit_L001_variance', 'HpHp_L1_weight', 'HH_jit_L3_variance', 'MI_dir_L5_variance', 'HH_L3_pcc', 'MI_dir_L3_variance', 'HH_jit_L3_weight', 'HH_L5_weight']\n","            Time taken for regular XGBoost feature selection = 3 seconds\n","        Selected: ['HH_jit_L001_variance', 'MI_dir_L3_variance', 'HH_L3_pcc', 'HH_jit_L3_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 18 important features:\n","['H_L001_mean', 'HH_L001_weight', 'H_L001_variance', 'MI_dir_L001_weight', 'HpHp_L001_weight', 'HH_jit_L001_variance', 'H_L5_mean', 'HH_L001_covariance', 'H_L01_variance', 'H_L01_mean', 'MI_dir_L5_variance', 'MI_dir_L3_variance', 'HH_jit_L01_weight', 'HH_jit_L3_weight', 'HpHp_L1_weight', 'HH_jit_L3_variance', 'HH_L3_pcc', 'HH_L5_weight']\n","Total Time taken for featurewiz selection = 30 seconds\n","Output contains a list of 18 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (8094, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (97) highly correlated variables:\n","    Following (18) vars selected: ['H_L001_weight', 'MI_dir_L001_weight', 'H_L001_mean', 'MI_dir_L01_weight', 'HpHp_L01_weight', 'HH_jit_L01_weight', 'H_L1_mean', 'MI_dir_L5_mean', 'MI_dir_L1_weight', 'HH_L001_covariance', 'MI_dir_L1_variance', 'H_L5_variance', 'MI_dir_L5_variance', 'HpHp_L5_weight', 'MI_dir_L3_variance', 'HH_L1_pcc', 'HH_jit_L001_variance', 'HH_jit_L3_variance']\n","Completed SULOV. 18 features selected\n","Time taken for SULOV method = 5 seconds\n","Finally 18 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L001_mean', 'H_L001_weight', 'MI_dir_L01_weight', 'MI_dir_L1_weight', 'H_L1_mean', 'HpHp_L5_weight']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['H_L1_mean', 'MI_dir_L5_mean', 'HH_jit_L01_weight', 'MI_dir_L1_variance', 'MI_dir_L1_weight', 'HH_jit_L001_variance', 'HpHp_L01_weight', 'HpHp_L5_weight', 'HH_jit_L3_variance', 'H_L5_variance', 'HH_L001_covariance', 'MI_dir_L3_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_L001_covariance', 'MI_dir_L1_weight', 'MI_dir_L1_variance', 'HH_jit_L001_variance', 'HpHp_L5_weight', 'HH_jit_L3_variance', 'HH_L1_pcc', 'MI_dir_L3_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_jit_L001_variance', 'HpHp_L5_weight', 'HH_jit_L3_variance', 'HH_L1_pcc', 'MI_dir_L5_variance', 'MI_dir_L3_variance']\n","            Time taken for regular XGBoost feature selection = 2 seconds\n","        Selected: ['HH_jit_L001_variance', 'HH_jit_L3_variance']\n","            Time taken for regular XGBoost feature selection = 4 seconds\n","    Completed XGBoost feature selection in 4 seconds\n","Selected 17 important features:\n","['H_L001_mean', 'H_L001_weight', 'MI_dir_L01_weight', 'MI_dir_L1_weight', 'H_L1_mean', 'HpHp_L5_weight', 'MI_dir_L5_mean', 'HH_jit_L01_weight', 'MI_dir_L1_variance', 'HH_jit_L001_variance', 'HpHp_L01_weight', 'HH_jit_L3_variance', 'H_L5_variance', 'HH_L001_covariance', 'MI_dir_L3_variance', 'HH_L1_pcc', 'MI_dir_L5_variance']\n","Total Time taken for featurewiz selection = 12 seconds\n","Output contains a list of 17 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (8094, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (94) highly correlated variables:\n","    Following (21) vars selected: ['H_L001_mean', 'H_L001_weight', 'H_L01_weight', 'H_L01_mean', 'HH_jit_L001_weight', 'HH_L001_weight', 'HpHp_L001_weight', 'HH_jit_L01_weight', 'H_L1_mean', 'MI_dir_L5_mean', 'MI_dir_L01_variance', 'MI_dir_L1_weight', 'HH_L001_pcc', 'MI_dir_L3_weight', 'H_L1_variance', 'H_L5_variance', 'MI_dir_L5_variance', 'HH_jit_L1_weight', 'HpHp_L5_pcc', 'HH_jit_L001_variance', 'HH_jit_L3_variance']\n","Completed SULOV. 21 features selected\n","Time taken for SULOV method = 5 seconds\n","Finally 21 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L001_mean']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['MI_dir_L01_variance', 'H_L1_mean', 'MI_dir_L1_weight', 'MI_dir_L5_mean', 'HpHp_L001_weight', 'MI_dir_L3_weight', 'H_L1_variance', 'H_L5_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['MI_dir_L01_variance', 'H_L1_mean', 'MI_dir_L1_weight', 'MI_dir_L5_mean', 'HH_jit_L001_variance', 'MI_dir_L3_weight', 'H_L1_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_L001_pcc', 'MI_dir_L3_weight', 'H_L1_variance', 'HH_jit_L001_variance', 'HH_jit_L1_weight', 'HH_jit_L3_variance', 'H_L5_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_jit_L001_variance', 'MI_dir_L5_variance', 'HH_jit_L1_weight', 'HH_jit_L3_variance', 'HpHp_L5_pcc']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 15 important features:\n","['H_L001_mean', 'MI_dir_L01_variance', 'H_L1_mean', 'MI_dir_L1_weight', 'MI_dir_L5_mean', 'HpHp_L001_weight', 'MI_dir_L3_weight', 'H_L1_variance', 'H_L5_variance', 'HH_jit_L001_variance', 'HH_L001_pcc', 'HH_jit_L1_weight', 'HH_jit_L3_variance', 'MI_dir_L5_variance', 'HpHp_L5_pcc']\n","Total Time taken for featurewiz selection = 7 seconds\n","Output contains a list of 15 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (8094, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (98) highly correlated variables:\n","    Following (17) vars selected: ['HH_jit_L3_weight', 'MI_dir_L001_mean', 'H_L001_mean', 'MI_dir_L001_weight', 'H_L001_weight', 'HH_L001_weight', 'MI_dir_L01_mean', 'MI_dir_L1_mean', 'HpHp_L001_weight', 'H_L01_variance', 'H_L5_weight', 'HH_jit_L5_weight', 'HpHp_L5_weight', 'H_L5_variance', 'MI_dir_L5_variance', 'HH_L1_pcc', 'HH_jit_L01_variance']\n","Completed SULOV. 17 features selected\n","Time taken for SULOV method = 7 seconds\n","Finally 17 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['MI_dir_L001_mean', 'MI_dir_L001_weight', 'HH_jit_L01_variance', 'H_L01_variance', 'MI_dir_L1_mean', 'HH_L001_weight', 'H_L5_variance', 'H_L5_weight']\n","            Time taken for regular XGBoost feature selection = 2 seconds\n","        Selected: ['MI_dir_L01_mean', 'MI_dir_L001_weight', 'H_L01_variance', 'H_L5_weight', 'HH_jit_L01_variance', 'HpHp_L001_weight', 'MI_dir_L1_mean', 'H_L5_variance']\n","            Time taken for regular XGBoost feature selection = 3 seconds\n","        Selected: ['MI_dir_L01_mean', 'HH_jit_L01_variance', 'H_L5_weight', 'H_L01_variance', 'HpHp_L001_weight', 'H_L5_variance', 'MI_dir_L1_mean']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['H_L01_variance', 'H_L5_weight', 'HH_jit_L01_variance', 'HpHp_L5_weight', 'H_L5_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_jit_L01_variance', 'HpHp_L5_weight', 'HH_L1_pcc', 'H_L5_variance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_jit_L01_variance', 'HH_L1_pcc']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 12 important features:\n","['MI_dir_L001_mean', 'MI_dir_L001_weight', 'HH_jit_L01_variance', 'H_L01_variance', 'MI_dir_L1_mean', 'HH_L001_weight', 'H_L5_variance', 'H_L5_weight', 'MI_dir_L01_mean', 'HpHp_L001_weight', 'HpHp_L5_weight', 'HH_L1_pcc']\n","Total Time taken for featurewiz selection = 13 seconds\n","Output contains a list of 12 important features and a train dataframe\n"]}],"source":["nodes = lerNode('Ecobee_Thermostat')\n","arrayDataDetec = []\n","arrayDataML = []\n","arrayAcuracias = []\n","arrayRecursos = []\n","arrayTempo = []\n","\n","#iniciado a variável que vai medir o tempo que demorou para fazer a seleção de atributos + separação de dados para ML e detecção\n","\n","for node in nodes:\n","  #dividindo os dados para o ML\n","  start_time = time.time()\n","\n","  varSF, varML = train_test_split(node, test_size=0.5, random_state=42)\n","\n","  array = varSF.to_numpy()\n","\n","  # Dividindo array em 5 partes iguais\n","  arrays_divididos = np.array_split(array, 5)\n","\n","  # Convertendo os arrays de volta para dataframes\n","  varSF, detec1, detec2, detec3, detec4 = [pd.DataFrame(arr, columns=varSF.columns) for arr in arrays_divididos]\n","\n","  #retorna um array com os melhores recursos\n","  atributosSelecionados = selecionarAtributos(varSF)\n","\n","  #retorna a acurácia e um array dos melhores recursos\n","  acuracia, recursos = medirAcuracia(varSF, atributosSelecionados)\n","\n","  arrayAcuracias.append(acuracia)\n","  arrayRecursos.append(recursos)\n","\n","  arrayDetecs = [detec1, detec2, detec3, detec4]\n","  arrayDataML.append(varML)\n","  arrayDataDetec.append(arrayDetecs)\n","\n","\n","  end_time = time.time()\n","  tempoSF = end_time - start_time\n","\n","  arrayTempo.append(tempoSF)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1687286680686,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"KoeCt4B9mL4b","outputId":"7200176e-3642-4f06-d301-1fb34ab2a1fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[36.68803358078003, 17.54189705848694, 10.08005666732788, 16.711519718170166]\n"]}],"source":["print(arrayTempo)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7IApYmpBRY9"},"outputs":[],"source":["\n","\n","# for node in nodes:\n","#   #dividindo os dados para o ML\n","#   varSF, varML = train_test_split(node, test_size=0.5, random_state=42)\n","\n","#   array = varSF.to_numpy()\n","\n","#   # Dividindo array em 5 partes iguais\n","#   arrays_divididos = np.array_split(array, 5)\n","\n","#   # Convertendo os arrays de volta para dataframes\n","#   varSF, detec1, detec2, detec3, detec4 = [pd.DataFrame(arr, columns=varSF.columns) for arr in arrays_divididos]\n","\n","#   #retorna um array com os melhores recursos\n","#   atributosSelecionados = selecionarAtributos(varSF)\n","\n","#   #retorna a acurácia e um array dos melhores recursos\n","#   acuracia, recursos = medirAcuracia(varSF, atributosSelecionados)\n","\n","#   arrayAcuracias.append(acuracia)\n","#   arrayRecursos.append(recursos)\n","\n","#   arrayDetecs = [detec1, detec2, detec3, detec4]\n","#   arrayDataML.append(varML)\n","#   arrayDataDetec.append(arrayDetecs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SkR5CIPNv8_"},"outputs":[],"source":["#se houver 2 arrays com a maior acurácia, é selecionado aquele que tem menos recursos.\n","maior_valor = max(arrayAcuracias)\n","if arrayAcuracias.count(maior_valor) >= 2:\n","    bestFeatures = menorArray(arrayRecursos)\n","else:\n","  posicao_maior_numero = arrayAcuracias.index(maior_valor)\n","  bestFeatures = arrayRecursos[posicao_maior_numero]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUXfoRUxQr5y"},"outputs":[],"source":["#adicionando a variável ataque para o ML\n","bestFeaturesAtaque = bestFeatures\n","bestFeaturesAtaque.append('ataque')\n","\n","#aplicando o array com os melhores recursos nos dados de ML e detecção\n","for i in range(4):\n","  df_to_update = arrayDataDetec[i]\n","  for j in range(4):\n","    df_to_update[j] = df_to_update[j].loc[:, bestFeaturesAtaque]\n","\n","for i in range(4):\n","  arrayDataML[i] = arrayDataML[i].loc[:, bestFeaturesAtaque]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGb1Oz6ZVJIN"},"outputs":[],"source":["#COMEÇANDO O APRENDIZADO FEDERADO\n","def criarDataset(dataFrame):\n","\n","  if 'ataque' in dataFrame.columns:\n","    target = dataFrame.pop('ataque')\n","    dataset = tf.data.Dataset.from_tensor_slices({'features': dataFrame.values, 'targets': target.values})\n","  else:\n","    numpy_array = np.array(dataFrame.values)\n","    dataset = tf.data.Dataset.from_tensor_slices(numpy_array)\n","\n","  return dataset\n","\n","def preprocess(dataset, batch_size, num_features):\n","\n","    # Função de formatação em lote\n","    def batch_format_fn(element):\n","\n","        return (\n","            tf.cast(tf.reshape(element['features'], [-1, num_features]), tf.float64),\n","            tf.cast(tf.reshape(element['targets'], [-1, 1]), tf.int64)\n","            )\n","\n","    # Aplicando a função de formatação em lote\n","\n","    return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzTd5bsd-vHZ"},"outputs":[],"source":["datasetsTrain = []\n","datasetTupla = []\n","\n","for i in range(4):\n","\n","  newDataSetTrain = criarDataset(arrayDataML[i])\n","\n","\n","\n","  datasetsTrain.append(newDataSetTrain)\n","\n","\n","#criando array que vai armazenar os Datasets\n","#pegando o número de features para a função preprocess\n","\n","preProcessedDataSets = []\n","num_features = arrayDataML[1].shape[1]\n","BATCH_SIZE = num_features * 5\n","\n","for i in range(4):\n","\n","  dataPreProcessed = preprocess(datasetsTrain[i], BATCH_SIZE, num_features)\n","\n","  preProcessedDataSets.append(dataPreProcessed)\n","\n","  tuplaClient = [dataPreProcessed]\n","\n","  datasetTupla.append(tuplaClient)\n","\n","\n","#finalizado a seleção de atributos e a separação de dados. tempoSF irá conter o tempo que durou o SF.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnrKVvwtKiOJ"},"outputs":[],"source":["# #VISUALIZANDO A QUANTIDADE DE DADOS TOTAL, QUANTIDADE DE DADOS PARA SF, ML, DETECÇÃO E TEMPO NECESSÁRIO\n","# num_elementos = 0\n","# print(\"quantidade de registros para SF: \" + str(len(varSF.index)))\n","\n","# print(\"#############################################\")\n","\n","# for i in range(len(arrayDataDetec)):\n","#   print(f\"quantidade de registros para detecção no node{i+1}: \")\n","#   for j in range(len(arrayDataDetec)):\n","#     print(str(len(arrayDataDetec[i][j].index)))\n","\n","# print(\"#############################################\")\n","\n","# for j in range(len(arrayDataML)):\n","#   print(f\"Quantidade de registros para ML no node{j+1}: \" +str(len(arrayDataML[j].index)))\n","\n","# print(\"#############################################\")\n","\n","# print(f\"Tempo necessário para SF e divisão dos dados: {tempoSF}\")\n","\n","# print(\"#############################################\")\n","\n","# for e in range(len(preProcessedDataSets)):\n","#   numbers = preProcessedDataSets[e].reduce(0, lambda x, _: x + 1).numpy()\n","#   num_elementos += numbers\n","# print(f\"número de dados para ML: {num_elementos}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7AzEhy2y22S"},"outputs":[],"source":["\n","element_spec = preProcessedDataSets[0].element_spec\n","\n","def create_keras_model():\n","\n","  return tf.keras.models.Sequential([\n","      tf.keras.layers.InputLayer(input_shape=(num_features,)),\n","      tf.keras.layers.Dense(40, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform()),\n","      tf.keras.layers.Dense(50, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform()),\n","      tf.keras.layers.Dense(units=1, activation='sigmoid', kernel_initializer=tf.keras.initializers.HeUniform())\n","  ])\n","\n","def model_fn():\n","\n","  keras_model = create_keras_model()\n","  return tff.learning.models.from_keras_model(\n","      keras_model,\n","      input_spec=element_spec,\n","      loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),\n","      metrics=[tf.keras.metrics.BinaryAccuracy()]\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0S7Iym0Awfne"},"outputs":[],"source":["mean = tff.aggregators.MeanFactory()\n","learningprocess = tff.learning.algorithms.build_weighted_fed_avg(\n","    model_fn,\n","    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),\n","    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1, momentum=0.9),\n","    model_aggregator = mean\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8049,"status":"ok","timestamp":1687286691528,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"fZIi3OMhQVzp","outputId":"ca7dac38-8f2f-4406-cac4-dddab48cc204"},"outputs":[{"name":"stdout","output_type":"stream","text":["round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9587637), ('loss', 0.05020117), ('num_examples', 161872), ('num_batches', 6476)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Binary Accuracy: 95.88\n","7.834642648696899\n"]}],"source":["server_state = learningprocess.initialize()\n","start_time = time.time()\n","server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","print('Binary Accuracy:', formatted_accuracy)\n","\n","end_time = time.time()\n","\n","total_time = end_time - start_time\n","print(total_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9012,"status":"ok","timestamp":1687286700525,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"qh81fghbIUQX","outputId":"7f222b85-f9ba-471f-8415-7cf7aadb967c"},"outputs":[{"name":"stdout","output_type":"stream","text":["round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95897996), ('loss', 0.049734704), ('num_examples', 40468), ('num_batches', 1619)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 95.90\n","2.2502682209014893\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9587575), ('loss', 0.05049706), ('num_examples', 40468), ('num_batches', 1619)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 95.88\n","2.2268545627593994\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95816445), ('loss', 0.050376743), ('num_examples', 40468), ('num_batches', 1619)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 95.82\n","2.1904704570770264\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95915294), ('loss', 0.050196175), ('num_examples', 40468), ('num_batches', 1619)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 95.92\n","2.1563990116119385\n"]}],"source":["modelosClientes = []\n","for i in range(4):\n","  server_state_client = learningprocess.initialize()\n","  start_time = time.time()\n","  server_state_client, metrics = learningprocess.next(server_state_client, datasetTupla[i])\n","  print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","  #visualizar apenas a acurácia\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","  print(f'Acurácia do cliente {i + 1}:', formatted_accuracy)\n","\n","  end_time = time.time()\n","\n","  total_time = end_time - start_time\n","  print(total_time)\n","\n","  modelosClientes.append(server_state)\n","\n","#round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.84064984), ('loss', 0.117992006), ('num_examples', 46903), ('num_batches', 1564)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DI6Kd_lALoPX"},"outputs":[],"source":["#função para testar o modelo\n","def predict_on_new_data(server_state, new_data):\n","\n","  keras_model = create_keras_model()\n","\n","  tf.nest.map_structure(\n","      lambda var, t: var.assign(t),\n","      keras_model.trainable_weights, server_state.global_model_weights.trainable)\n","  metric = tf.keras.metrics.BinaryAccuracy()\n","\n","\n","  predictions = keras_model.predict(new_data)\n","  return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ePtMvyNx1MJ"},"outputs":[],"source":["def preprocessDATADETEC(dataset, batch_size, num_features):\n","\n","    # Função de formatação em lote\n","    def batch_format_fn(element):\n","\n","        return tf.reshape(element, [-1, num_features])\n","\n","    # Aplicando a função de formatação em lote\n","    return dataset.batch(batch_size).map(batch_format_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145178,"status":"ok","timestamp":1687286845690,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"7osOdLXAB6XB","outputId":"2ce396d7-48c3-41f4-d294-61899cf407b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 0s 1ms/step\n","324/324 [==============================] - 0s 1ms/step\n","324/324 [==============================] - 1s 2ms/step\n","round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9592274), ('loss', 0.050196785), ('num_examples', 194248), ('num_batches', 7772)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 95.92\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.960257), ('loss', 0.049638595), ('num_examples', 48562), ('num_batches', 1943)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 96.03\n","1.7489759922027588\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95821834), ('loss', 0.05065223), ('num_examples', 48562), ('num_batches', 1943)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 95.82\n","1.7000243663787842\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95920676), ('loss', 0.050253756), ('num_examples', 48562), ('num_batches', 1943)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 95.92\n","1.7260096073150635\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95906264), ('loss', 0.05026692), ('num_examples', 48562), ('num_batches', 1943)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 95.91\n","1.7181761264801025\n","A quantidade de dados usado nessa rodada foi de: 7772\n","tempo de treino modelo geral 1  26.06230092048645\n","324/324 [==============================] - 0s 1ms/step\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 1s 2ms/step\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9591701), ('loss', 0.05023462), ('num_examples', 226623), ('num_batches', 9068)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 95.92\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9597395), ('loss', 0.049805503), ('num_examples', 56656), ('num_batches', 2267)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 95.97\n","5.077801942825317\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95868045), ('loss', 0.050543845), ('num_examples', 56656), ('num_batches', 2267)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 95.87\n","3.0487442016601562\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95933354), ('loss', 0.05012905), ('num_examples', 56656), ('num_batches', 2267)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 95.93\n","2.918452024459839\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95887387), ('loss', 0.05046908), ('num_examples', 56655), ('num_batches', 2267)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 95.89\n","2.9504928588867188\n","A quantidade de dados usado nessa rodada foi de: 9068\n","tempo de treino modelo geral 2  21.8362033367157\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 0s 1ms/step\n","324/324 [==============================] - 0s 1ms/step\n","324/324 [==============================] - 1s 1ms/step\n","round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9593776), ('loss', 0.050200887), ('num_examples', 258995), ('num_batches', 10364)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 95.94\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95938164), ('loss', 0.050212756), ('num_examples', 64749), ('num_batches', 2591)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 95.94\n","2.257802724838257\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95901096), ('loss', 0.05042251), ('num_examples', 64749), ('num_batches', 2591)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 95.90\n","2.24355411529541\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9599994), ('loss', 0.04976094), ('num_examples', 64749), ('num_batches', 2591)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 96.00\n","2.3201711177825928\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95908755), ('loss', 0.05041253), ('num_examples', 64748), ('num_batches', 2591)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 95.91\n","2.8612070083618164\n","A quantidade de dados usado nessa rodada foi de: 10364\n","tempo de treino modelo geral 3  27.20946955680847\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 1s 2ms/step\n","324/324 [==============================] - 0s 1ms/step\n","round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95955616), ('loss', 0.05012293), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 95.96\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9594739), ('loss', 0.05016744), ('num_examples', 72842), ('num_batches', 2915)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 95.95\n","3.5953681468963623\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95939153), ('loss', 0.050238494), ('num_examples', 72842), ('num_batches', 2915)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 95.94\n","3.570408344268799\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9602839), ('loss', 0.04962373), ('num_examples', 72842), ('num_batches', 2915)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 96.03\n","2.6297953128814697\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9590615), ('loss', 0.050462697), ('num_examples', 72841), ('num_batches', 2915)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 95.91\n","2.554800271987915\n","A quantidade de dados usado nessa rodada foi de: 11660\n","tempo de treino modelo geral 4  27.10141396522522\n"]}],"source":["#simulando a detecção\n","qtDataDatasets = 0\n","for i in range(4):\n","  start_time = time.time()\n","  for j in range(4):\n","\n","    #pegando o dataframe correto e transformando-o em um dataset sem rótulos\n","    data = arrayDataDetec[j][i]\n","    varAtaque = data.pop('ataque')\n","    dataset = criarDataset(data)\n","\n","    #usando o modelo global para fazer previsões\n","    preprocessedDataSet = preprocessDATADETEC(dataset, BATCH_SIZE, num_features)\n","    returned = predict_on_new_data(server_state, preprocessedDataSet)\n","\n","    #transformando o dataset em um array numpy\n","    numpyDataSet = np.array(list(dataset.as_numpy_iterator()))\n","\n","    #adicionando os valores que o modelo previu com os dados que ele utilizou\n","    result = np.concatenate((numpyDataSet, returned), axis=1)\n","\n","    #transformando o array numpy em um dataframe\n","    df = pd.DataFrame(result, columns=[bestFeatures])\n","\n","    #adicionando os valores reais para comparação\n","    df['valoresReais'] = varAtaque\n","\n","    #SUPOSIÇÃO DE QUE A PARTIR DAQUI, O TÉCNICO JÁ ARRUMOU O QUE TINHA QUE ARRUMAR E RETORNOU O DATASET\n","    #CÓDIGO QUE SIMULA O TRABALHO DO TÉCNICO\n","    df = df.drop('ataque', axis=1)\n","\n","    df = df.rename(columns={'valoresReais': 'ataque'})\n","\n","    df['ataque'] = df['ataque'].astype(int)\n","\n","    #######################\n","    #gerando um novo dataset com os dados corretos para treinar o modelo novamente\n","    dataset = criarDataset(df)\n","\n","    dataSetPreProcess = preprocess(dataset, BATCH_SIZE, num_features)\n","\n","\n","    #convertendo em um dataset\n","    preProcessedDataSets[i] = tf.data.Dataset.concatenate(preProcessedDataSets[i], dataSetPreProcess)\n","    datasetTupla[j][0] = tf.data.Dataset.concatenate(datasetTupla[j][0], dataSetPreProcess)\n","\n","#treinando e atualizando os pesos dos clientes\n","  for round in range(4):\n","    nm_preProcessedDataSets = preProcessedDataSets[round].reduce(0, lambda x, _: x + 1).numpy()\n","    qtDataDatasets += nm_preProcessedDataSets\n","\n","  server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","  print('round {:2d}, metrics={}'.format(i, metrics))\n","\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","  print('Acurácia do Modelo Geral:', formatted_accuracy)\n","\n","  end_time = time.time()\n","\n","  tempoDetec = end_time - start_time\n","\n","  #treinando os clientes individualmente\n","  for g in range(4):\n","    start_time2 = time.time()\n","    modelosClientes[g], metrics = learningprocess.next(modelosClientes[g], datasetTupla[g])\n","    print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","    #visualizar apenas a acurácia\n","    binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","    formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","    print(f'Acurácia do cliente {i + 1}:', formatted_accuracy)\n","\n","    end_time2 = time.time()\n","\n","    total_time2 = end_time2 - start_time2\n","    print(total_time2)\n","\n","    modelosClientes[g] = server_state\n","  #testanto o modelo\n","  print(f\"A quantidade de dados usado nessa rodada foi de: {qtDataDatasets}\")\n","\n","\n","  qtDataDatasets = 0\n","  print(f\"tempo de treino modelo geral {i + 1}  {tempoDetec}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168922,"status":"ok","timestamp":1687287014605,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"6RFFAGdMns5q","outputId":"3e840fd1-a2fd-4484-caa2-d5e641e1a473"},"outputs":[{"name":"stdout","output_type":"stream","text":["round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95893836), ('loss', 0.05019766), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95900357), ('loss', 0.050205577), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9590997), ('loss', 0.05019089), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95921636), ('loss', 0.050173342), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95932275), ('loss', 0.050156903), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95938456), ('loss', 0.05014881), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95947725), ('loss', 0.0501343), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.95954585), ('loss', 0.05012454), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9595596), ('loss', 0.050122358), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 10, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 11, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 12, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 13, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 14, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 15, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.959563), ('loss', 0.050122045), ('num_examples', 291367), ('num_batches', 11660)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","168.73781991004944\n"]}],"source":["server_state = learningprocess.initialize()\n","\n","start_time = time.time()\n","\n","bestAccuracy = 0\n","best_server_state = 0\n","\n","for i in range(16):\n","  server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  print('round {:2d}, metrics={}'.format(i, metrics))\n","\n","  if binary_accuracy > bestAccuracy:\n","    bestAccuracy = binary_accuracy\n","    best_server_state = server_state\n","  elif i > 5 and binary_accuracy < bestAccuracy:\n","    print(f\"Rodada em que o modelo convergiu: {i}\")\n","    break\n","\n","end_time = time.time()\n","\n","total_time = end_time - start_time\n","print(total_time)"]},{"cell_type":"markdown","metadata":{"id":"5LL2YyABDNXm"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pY5r2cKBnGu1"},"outputs":[],"source":["\n","# start_time = time.time()\n","# for round in range(20):\n","#     server_state = federated_algorithm.next(server_state, preProcessedDataSets)\n","\n","# #testanto o modelo\n","# evaluate(server_state)\n","\n","# end_time = time.time()\n","# tempoExtra = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687287014607,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"8Od_uc54STs-","outputId":"02e3f1f6-5c89-4d25-e99b-07558f47b9ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["16.711519718170166\n"]}],"source":["print(tempoSF)\n","# print(tempoDetec)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1687287014607,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"},"user_tz":180},"id":"RcAnkL2nSuIF","outputId":"8b75338b-c3d9-42fe-f926-71db61083eef"},"outputs":[{"name":"stdout","output_type":"stream","text":["O número total de registros é 323743.\n","Dados para ML 161872\n","25\n"]}],"source":["# Contando o número de registros em todos os dataframes\n","num_registros = sum(df.shape[0] for df in nodes)\n","\n","# Exibindo o número total de registros\n","print(f\"O número total de registros é {num_registros}.\")\n","\n","num_registros = sum(df.shape[0] for df in arrayDataML)\n","\n","print(f\"Dados para ML {num_registros}\")\n","\n","print(BATCH_SIZE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pks-5CCFTn0a"},"outputs":[],"source":["# print(nodes[0].shape)\n","# print(nodes[1].shape)\n","# print(nodes[2].shape)\n","# print(nodes[3].shape)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1dNsmXVhktlMmJirIsJiTblxyZfwTC_8E","timestamp":1687197578844},{"file_id":"1vN_N9m4UMFF8vtBgZREX2HH6isPsRFVh","timestamp":1684517423398},{"file_id":"146EF9yzd1aG_qatqFMoNg8-KkWHHUpxT","timestamp":1684161661680}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

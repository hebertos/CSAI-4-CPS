{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f0F1zJxI53HF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278137050,"user_tz":180,"elapsed":20602,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"8bc831e2-2653-4ca9-ab56-a7ff73211190"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/AutoViML/featurewiz.git\n","  Cloning https://github.com/AutoViML/featurewiz.git to /tmp/pip-req-build-c30rnfp8\n","  Running command git clone --filter=blob:none --quiet https://github.com/AutoViML/featurewiz.git /tmp/pip-req-build-c30rnfp8\n","  Resolved https://github.com/AutoViML/featurewiz.git to commit 2fab4170382b698114935f7b0842f36ebcaa51c0\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (7.34.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (1.0.0)\n","Requirement already satisfied: xgboost~=1.5 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (1.7.5)\n","Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (3.7.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (0.12.2)\n","Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (1.2.2)\n","Requirement already satisfied: networkx>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2.8.3)\n","Requirement already satisfied: category_encoders>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2.6.1)\n","Requirement already satisfied: xlrd>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2.0.1)\n","Requirement already satisfied: imbalanced-learn>=0.7 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (0.10.1)\n","Requirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (4.65.0)\n","Requirement already satisfied: dask>=2021.11.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2022.12.1)\n","Requirement already satisfied: lightgbm>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (3.3.5)\n","Requirement already satisfied: distributed>=2021.11.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2022.12.1)\n","Requirement already satisfied: feather-format>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (0.4.1)\n","Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (9.0.0)\n","Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (2023.4.0)\n","Requirement already satisfied: Pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from featurewiz==0.3.1) (9.5.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.4.0->featurewiz==0.3.1) (1.22.4)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.4.0->featurewiz==0.3.1) (1.7.3)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.4.0->featurewiz==0.3.1) (0.13.5)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.4.0->featurewiz==0.3.1) (0.5.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (8.1.3)\n","Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (2.2.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (22.0)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (1.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (6.0)\n","Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask>=2021.11.0->featurewiz==0.3.1) (0.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (3.1.2)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (1.0.0)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (1.0.5)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (5.9.5)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (1.7.0)\n","Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (6.3.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (1.26.15)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2021.11.0->featurewiz==0.3.1) (3.0.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.7->featurewiz==0.3.1) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.7->featurewiz==0.3.1) (3.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm>=3.2.1->featurewiz==0.3.1) (0.40.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.4->featurewiz==0.3.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.4->featurewiz==0.3.1) (2022.7.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (0.18.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (3.0.38)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->featurewiz==0.3.1) (4.8.0)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (6.4.8)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (5.4.3)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (6.5.4)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (5.5.6)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->featurewiz==0.3.1) (7.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->featurewiz==0.3.1) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->featurewiz==0.3.1) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->featurewiz==0.3.1) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->featurewiz==0.3.1) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->featurewiz==0.3.1) (3.0.9)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->featurewiz==0.3.1) (0.8.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders>=2.4.0->featurewiz==0.3.1) (1.16.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->featurewiz==0.3.1) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->featurewiz==0.3.1) (0.2.6)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->featurewiz==0.3.1) (0.2.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->featurewiz==0.3.1) (6.1.12)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->featurewiz==0.3.1) (3.6.4)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->featurewiz==0.3.1) (3.0.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->distributed>=2021.11.0->featurewiz==0.3.1) (2.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (4.9.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (6.0.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (0.4)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (5.3.0)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (0.2.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (0.7.4)\n","Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (5.8.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->featurewiz==0.3.1) (1.2.1)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (23.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (21.3.0)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (1.5.6)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (0.17.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->featurewiz==0.3.1) (0.16.0)\n","Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->featurewiz==0.3.1) (2.3.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->featurewiz==0.3.1) (3.3.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->featurewiz==0.3.1) (2.16.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->featurewiz==0.3.1) (4.3.3)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->featurewiz==0.3.1) (21.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->featurewiz==0.3.1) (2.4.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->featurewiz==0.3.1) (0.5.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->featurewiz==0.3.1) (21.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->featurewiz==0.3.1) (0.19.3)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->featurewiz==0.3.1) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->featurewiz==0.3.1) (2.21)\n"]}],"source":["!pip install --quiet --upgrade tensorflow-federated\n","!pip install git+https://github.com/AutoViML/featurewiz.git\n","# !pip install featurewiz --ignore-installed --no-deps\n","# !pip install xlrd --ignore-installed --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoVjLHT7nb5Y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6fdb2d4-bf46-4130-9b30-b925f10df513","executionInfo":{"status":"ok","timestamp":1687278146044,"user_tz":180,"elapsed":8999,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Imported 0.3.2 version. Select nrows to a small number when running on huge datasets.\n","output = featurewiz(dataname, target, corr_limit=0.90, verbose=2, sep=',', \n","\t\theader=0, test_data='',feature_engg='', category_encoders='',\n","\t\tdask_xgboost_flag=False, nrows=None, skip_sulov=False, skip_xgboost=False)\n","Create new features via 'feature_engg' flag : ['interactions','groupby','target']\n","\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow_federated as tff\n","import featurewiz as FW\n","import tensorflow as tf\n","from google.colab import drive\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif, RFECV, RFE\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.metrics import accuracy_score, explained_variance_score\n","from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC, LinearSVC\n","import pickle\n","import inspect\n","from tensorflow import keras\n","import time\n","import joblib\n","import attr\n","import matplotlib.pyplot as plt\n","import collections\n","import sys\n","import nest_asyncio\n","import math\n","\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omT-AsBDnxrg"},"outputs":[],"source":["def selectFW(dataset):\n","\n","  out1, out2 = FW.featurewiz(dataname=dataset, target='ataque', corr_limit=0.40, verbose=0, sep=',',\n","      header=0, test_data='',feature_engg='', category_encoders='',\n","      dask_xgboost_flag=False, nrows=None)\n","\n","  return out1, out2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SL0wGRFUoUMf"},"outputs":[],"source":["def metFilter(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  #f_regression\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('selector', SelectKBest(f_classif, k=10)), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['selector'].get_support()]\n","  df_selected = pd.DataFrame(pipe.named_steps['selector'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","  #WRAPPER\n","def metWrapper(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  estimator = LogisticRegression()\n","\n","  # Criando um seletor de recursos com RFE\n","  selector = RFE(estimator, n_features_to_select=10, step=1)\n","\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('rfe', selector), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  # Executando a seleção de recursos\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['rfe'].get_support()]\n","\n","  df_selected = pd.DataFrame(pipe.named_steps['rfe'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","def metIntrinseco(dataFrame):\n","  X = dataFrame.drop('ataque', axis=1)\n","  y = dataFrame['ataque']\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  rfc = RandomForestClassifier(n_estimators=100, random_state=42, max_features=10)\n","\n","  pipe = Pipeline([('scaler', StandardScaler()), #normalização\n","    ('rfc', SelectFromModel(rfc, threshold=-np.inf, max_features=10)), #selecionar os 10 melhores\n","    ('classifier', SVC(kernel='linear', C=1))]) #classificação\n","\n","  pipe.fit(X_train, y_train)\n","  score = pipe.score(X_test, y_test)\n","\n","  selected_features = X.columns[pipe.named_steps['rfc'].get_support()]\n","  df_selected = pd.DataFrame(pipe.named_steps['rfc'].transform(X_train), columns=selected_features)\n","\n","  selected_features = selected_features.tolist()\n","\n","  return selected_features\n","\n","def metResult(arrayFilter, arrayWrapper,arrayIntrinseco):\n","  arrayResult = []\n","  for f in range(len(arrayFilter)):\n","    for w in range(len(arrayWrapper)):\n","      for i in range(len(arrayIntrinseco)):\n","        if ((arrayFilter[f] == arrayWrapper[w]) and (arrayFilter[f] == arrayIntrinseco[i])):\n","          arrayResult.append(arrayFilter[i])\n","  return arrayResult\n","\n","def selecionarAtributos(dataset):\n","    features, dataset = selectFW(dataset)\n","\n","    arrayFilter = metFilter(dataset)\n","    arrayWrapper = metWrapper(dataset)\n","    arrayIntrinseco = metIntrinseco(dataset)\n","\n","    arrayResult = metResult(arrayFilter,arrayWrapper,arrayIntrinseco)\n","\n","    arrayMetodos = [arrayResult, arrayFilter, arrayWrapper, arrayIntrinseco]\n","\n","    arrayMetodos = [arr for arr in arrayMetodos if len(arr) > 0]\n","\n","    return arrayMetodos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuBf5QQqA_1Q"},"outputs":[],"source":["#mede a acurácia de todos os atributos e retornando o com maior acurácia\n","def medirAcuracia(dataFrame, arrayRecursos):\n","  maiorAcuracia = 0\n","  melhoresRecursos = []\n","  for recursos in arrayRecursos:\n","\n","    array = np.append(recursos, 'ataque')\n","    newDataFrame = dataFrame.loc[:, array]\n","\n","    X = newDataFrame.drop('ataque', axis=1)\n","    y = newDataFrame['ataque']\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","    #treinamento\n","    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    rf.fit(X_train, y_train)\n","\n","    y_pred = rf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    if accuracy > maiorAcuracia:\n","      maiorAcuracia = accuracy\n","      melhoresRecursos = recursos\n","\n","  return maiorAcuracia, melhoresRecursos\n","\n","def menorArray(array):\n","  min_length = float('inf')  # inicializa o valor mínimo como infinito\n","  min_array = None  # inicializa o sub-array correspondente como vazio\n","\n","  for sub_array in array:\n","      if len(sub_array) < min_length:\n","          min_length = len(sub_array)\n","          min_array = sub_array\n","  return min_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jA1mO6Shjaam"},"outputs":[],"source":["def lerNode(datasetName):\n","  node1 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node1.csv')\n","  node2 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node2.csv')\n","  node3 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node3.csv')\n","  node4 = pd.read_csv(f'/content/drive/My Drive/nodes/{datasetName}/Node4.csv')\n","\n","  nodesArray = [node1,node2,node3,node4]\n","\n","  return nodesArray"]},{"cell_type":"code","source":["nodes = lerNode('Danmini_Doorbell2')\n","arrayDataDetec = []\n","arrayDataML = []\n","arrayAcuracias = []\n","arrayRecursos = []\n","arrayTempo = []\n","\n","#iniciado a variável que vai medir o tempo que demorou para fazer a seleção de atributos + separação de dados para ML e detecção\n","\n","for node in nodes:\n","  #dividindo os dados para o ML\n","  start_time = time.time()\n","\n","  varSF, varML = train_test_split(node, test_size=0.5, random_state=42)\n","\n","  array = varSF.to_numpy()\n","\n","  # Dividindo array em 5 partes iguais\n","  arrays_divididos = np.array_split(array, 5)\n","\n","  # Convertendo os arrays de volta para dataframes\n","  varSF, detec1, detec2, detec3, detec4 = [pd.DataFrame(arr, columns=varSF.columns) for arr in arrays_divididos]\n","\n","  #retorna um array com os melhores recursos\n","  atributosSelecionados = selecionarAtributos(varSF)\n","\n","  #retorna a acurácia e um array dos melhores recursos\n","  acuracia, recursos = medirAcuracia(varSF, atributosSelecionados)\n","\n","  arrayAcuracias.append(acuracia)\n","  arrayRecursos.append(recursos)\n","\n","  arrayDetecs = [detec1, detec2, detec3, detec4]\n","  arrayDataML.append(varML)\n","  arrayDataDetec.append(arrayDetecs)\n","\n","\n","  end_time = time.time()\n","  tempoSF = end_time - start_time\n","\n","  arrayTempo.append(tempoSF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiJ3ykpPyWDz","executionInfo":{"status":"ok","timestamp":1687278249376,"user_tz":180,"elapsed":103337,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"75858226-cb13-47cf-9177-9a55065e784a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (17542, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","Classifying features using a random sample of 10000 rows from dataset...\n","#### Single_Label Binary_Classification problem ####\n","    loading a random sample of 10000 rows into pandas for EDA\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (97) highly correlated variables:\n","    Following (18) vars selected: ['H_L3_weight', 'MI_dir_L3_weight', 'H_L01_weight', 'MI_dir_L5_weight', 'H_L1_weight', 'H_L001_variance', 'HH_jit_L3_mean', 'HH_jit_L5_mean', 'HH_L001_weight', 'MI_dir_L01_variance', 'HpHp_L01_weight', 'HpHp_L001_std', 'HH_L001_radius', 'HH_L1_std', 'HH_L001_pcc', 'HH_jit_L3_weight', 'HH_L1_pcc', 'HH_jit_L01_variance']\n","Completed SULOV. 18 features selected\n","Time taken for SULOV method = 8 seconds\n","Finally 18 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L3_weight', 'H_L1_weight', 'H_L01_weight', 'MI_dir_L5_weight']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['H_L1_weight', 'H_L001_variance', 'HpHp_L001_std', 'HpHp_L01_weight', 'HH_L001_radius', 'HH_jit_L3_weight']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_L001_pcc', 'HH_L001_weight', 'HpHp_L01_weight', 'MI_dir_L01_variance', 'HH_jit_L3_weight', 'HH_L001_radius', 'HH_jit_L01_variance']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HH_L001_pcc', 'HH_jit_L01_variance', 'HH_L1_pcc', 'HH_L001_radius', 'HH_jit_L3_weight', 'HH_L1_std']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HH_jit_L01_variance', 'HH_L1_pcc']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 15 important features:\n","['H_L3_weight', 'H_L1_weight', 'H_L01_weight', 'MI_dir_L5_weight', 'H_L001_variance', 'HpHp_L001_std', 'HpHp_L01_weight', 'HH_L001_radius', 'HH_jit_L3_weight', 'HH_L001_pcc', 'HH_L001_weight', 'MI_dir_L01_variance', 'HH_jit_L01_variance', 'HH_L1_pcc', 'HH_L1_std']\n","Total Time taken for featurewiz selection = 10 seconds\n","Output contains a list of 15 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (17542, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","Classifying features using a random sample of 10000 rows from dataset...\n","#### Single_Label Binary_Classification problem ####\n","    loading a random sample of 10000 rows into pandas for EDA\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (94) highly correlated variables:\n","    Following (21) vars selected: ['H_L001_weight', 'H_L3_weight', 'MI_dir_L001_weight', 'H_L1_weight', 'MI_dir_L001_mean', 'MI_dir_L5_weight', 'H_L001_variance', 'H_L01_mean', 'HpHp_L01_magnitude', 'HpHp_L5_magnitude', 'HpHp_L001_weight', 'MI_dir_L3_mean', 'HpHp_L001_std', 'HH_L001_radius', 'HH_L1_std', 'HH_L01_covariance', 'HH_L1_covariance', 'HpHp_L001_covariance', 'HpHp_L5_pcc', 'HpHp_L1_pcc', 'HpHp_L3_covariance']\n","Completed SULOV. 21 features selected\n","Time taken for SULOV method = 8 seconds\n","Finally 21 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L001_weight', 'MI_dir_L5_weight', 'H_L1_weight']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['MI_dir_L5_weight', 'HH_L001_radius', 'HpHp_L001_std', 'MI_dir_L001_mean', 'H_L01_mean', 'HpHp_L001_weight', 'H_L001_variance', 'HH_L1_std']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HpHp_L001_covariance', 'MI_dir_L3_mean', 'HH_L001_radius', 'HpHp_L001_weight', 'HpHp_L01_magnitude', 'HpHp_L001_std', 'HH_L1_std']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HpHp_L001_covariance', 'HH_L001_radius', 'HH_L1_std', 'HpHp_L001_std', 'HH_L01_covariance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HpHp_L001_covariance', 'HH_L1_covariance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 15 important features:\n","['H_L001_weight', 'MI_dir_L5_weight', 'H_L1_weight', 'HH_L001_radius', 'HpHp_L001_std', 'MI_dir_L001_mean', 'H_L01_mean', 'HpHp_L001_weight', 'H_L001_variance', 'HH_L1_std', 'HpHp_L001_covariance', 'MI_dir_L3_mean', 'HpHp_L01_magnitude', 'HH_L01_covariance', 'HH_L1_covariance']\n","Total Time taken for featurewiz selection = 10 seconds\n","Output contains a list of 15 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (17542, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","Classifying features using a random sample of 10000 rows from dataset...\n","#### Single_Label Binary_Classification problem ####\n","    loading a random sample of 10000 rows into pandas for EDA\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (92) highly correlated variables:\n","    Following (23) vars selected: ['MI_dir_L001_variance', 'H_L001_variance', 'H_L001_mean', 'MI_dir_L01_mean', 'MI_dir_L01_weight', 'H_L5_weight', 'MI_dir_L5_weight', 'HH_L01_weight', 'HH_L001_weight', 'HH_jit_L5_mean', 'MI_dir_L01_variance', 'HpHp_L001_weight', 'MI_dir_L5_mean', 'HpHp_L001_std', 'HH_L001_radius', 'HH_L1_std', 'HH_L01_pcc', 'HH_jit_L5_weight', 'HH_L1_covariance', 'HH_L5_pcc', 'HpHp_L3_pcc', 'HpHp_L5_covariance', 'HpHp_L1_covariance']\n","Completed SULOV. 23 features selected\n","Time taken for SULOV method = 8 seconds\n","Finally 23 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['MI_dir_L01_weight', 'MI_dir_L001_variance', 'HH_jit_L5_mean', 'HpHp_L001_weight', 'MI_dir_L01_variance', 'H_L5_weight', 'HH_L001_radius']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['H_L5_weight', 'HH_jit_L5_weight', 'HpHp_L001_weight', 'MI_dir_L01_variance', 'HH_jit_L5_mean', 'HH_L001_weight']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['MI_dir_L5_mean', 'HpHp_L001_weight', 'MI_dir_L01_variance', 'HH_L001_radius', 'HH_jit_L5_weight', 'HH_L1_std']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HH_L01_pcc', 'HH_L1_std', 'HH_jit_L5_weight', 'HH_L1_covariance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","        Selected: ['HpHp_L1_covariance', 'HpHp_L3_pcc', 'HpHp_L5_covariance']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 16 important features:\n","['MI_dir_L01_weight', 'MI_dir_L001_variance', 'HH_jit_L5_mean', 'HpHp_L001_weight', 'MI_dir_L01_variance', 'H_L5_weight', 'HH_L001_radius', 'HH_jit_L5_weight', 'HH_L001_weight', 'MI_dir_L5_mean', 'HH_L1_std', 'HH_L01_pcc', 'HH_L1_covariance', 'HpHp_L1_covariance', 'HpHp_L3_pcc', 'HpHp_L5_covariance']\n","Total Time taken for featurewiz selection = 11 seconds\n","Output contains a list of 16 important features and a train dataframe\n","featurewiz has selected 0.4 as the correlation limit. Change this limit to fit your needs...\n","Skipping feature engineering since no feature_engg input...\n","Skipping category encoding since no category encoders specified in input...\n","#### Single_Label Binary_Classification problem ####\n","    Loaded train data. Shape = (17542, 116)\n","    Some column names had special characters which were removed...\n","#### Single_Label Binary_Classification problem ####\n","No test data filename given...\n","Classifying features using a random sample of 10000 rows from dataset...\n","#### Single_Label Binary_Classification problem ####\n","    loading a random sample of 10000 rows into pandas for EDA\n","#######################################################################################\n","######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n","#######################################################################################\n","        No variables were removed since no ID or low-information variables found in data set\n","Removing 0 columns from further processing since ID or low information variables\n","#######################################################################################\n","#####  Searching for Uncorrelated List Of Variables (SULOV) in 115 features ############\n","#######################################################################################\n","    there are no null values in dataset...\n","    Removing (88) highly correlated variables:\n","    Following (27) vars selected: ['MI_dir_L001_variance', 'H_L001_mean', 'H_L001_variance', 'H_L01_mean', 'H_L5_weight', 'MI_dir_L5_weight', 'MI_dir_L001_weight', 'HpHp_L1_magnitude', 'HpHp_L3_magnitude', 'HH_jit_L3_mean', 'HH_jit_L01_weight', 'HpHp_L01_weight', 'HH_jit_L01_mean', 'HpHp_L001_std', 'HH_L001_radius', 'HH_L1_std', 'HH_L001_pcc', 'HH_L5_weight', 'HH_L1_pcc', 'HH_jit_L001_variance', 'HH_L3_pcc', 'HpHp_L5_pcc', 'HpHp_L3_pcc', 'HH_jit_L1_variance', 'HpHp_L01_pcc', 'HpHp_L01_covariance', 'HpHp_L1_pcc']\n","Completed SULOV. 27 features selected\n","Time taken for SULOV method = 9 seconds\n","Finally 27 vars selected after SULOV\n","Converting all features to numeric before sending to XGBoost...\n","    Number of booster rounds = 100\n","        Selected: ['H_L5_weight', 'MI_dir_L001_weight', 'MI_dir_L001_variance', 'HH_jit_L01_weight', 'HH_L5_weight', 'H_L001_mean']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['MI_dir_L5_weight', 'MI_dir_L001_weight', 'HpHp_L1_magnitude', 'HH_jit_L001_variance', 'HH_jit_L1_variance', 'HH_jit_L01_weight']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HH_L001_pcc', 'HpHp_L01_covariance', 'HH_jit_L01_weight', 'HpHp_L01_weight', 'HH_jit_L01_mean', 'HH_jit_L001_variance', 'HH_L1_pcc', 'HH_L5_weight', 'HH_L001_radius', 'HH_L1_std']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HH_L001_pcc', 'HpHp_L01_covariance', 'HH_jit_L001_variance', 'HH_L1_pcc', 'HH_L5_weight', 'HpHp_L01_pcc', 'HH_L1_std', 'HH_jit_L1_variance']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HpHp_L01_covariance', 'HH_jit_L1_variance', 'HpHp_L1_pcc', 'HH_L3_pcc', 'HpHp_L01_pcc']\n","            Time taken for regular XGBoost feature selection = 1 seconds\n","        Selected: ['HpHp_L01_covariance', 'HpHp_L1_pcc']\n","            Time taken for regular XGBoost feature selection = 0 seconds\n","    Completed XGBoost feature selection in 0 seconds\n","Selected 20 important features:\n","['H_L5_weight', 'MI_dir_L001_weight', 'MI_dir_L001_variance', 'HH_jit_L01_weight', 'HH_L5_weight', 'H_L001_mean', 'MI_dir_L5_weight', 'HpHp_L1_magnitude', 'HH_jit_L001_variance', 'HH_jit_L1_variance', 'HH_L001_pcc', 'HpHp_L01_covariance', 'HpHp_L01_weight', 'HH_jit_L01_mean', 'HH_L1_pcc', 'HH_L001_radius', 'HH_L1_std', 'HpHp_L01_pcc', 'HpHp_L1_pcc', 'HH_L3_pcc']\n","Total Time taken for featurewiz selection = 12 seconds\n","Output contains a list of 20 important features and a train dataframe\n"]}]},{"cell_type":"code","source":["print(arrayTempo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoeCt4B9mL4b","executionInfo":{"status":"ok","timestamp":1687278249377,"user_tz":180,"elapsed":20,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"de014512-ec23-4e02-8f96-ff0d7d8940e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[15.58984923362732, 15.397523641586304, 15.787748098373413, 18.447888374328613]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7IApYmpBRY9"},"outputs":[],"source":["\n","\n","# for node in nodes:\n","#   #dividindo os dados para o ML\n","#   varSF, varML = train_test_split(node, test_size=0.5, random_state=42)\n","\n","#   array = varSF.to_numpy()\n","\n","#   # Dividindo array em 5 partes iguais\n","#   arrays_divididos = np.array_split(array, 5)\n","\n","#   # Convertendo os arrays de volta para dataframes\n","#   varSF, detec1, detec2, detec3, detec4 = [pd.DataFrame(arr, columns=varSF.columns) for arr in arrays_divididos]\n","\n","#   #retorna um array com os melhores recursos\n","#   atributosSelecionados = selecionarAtributos(varSF)\n","\n","#   #retorna a acurácia e um array dos melhores recursos\n","#   acuracia, recursos = medirAcuracia(varSF, atributosSelecionados)\n","\n","#   arrayAcuracias.append(acuracia)\n","#   arrayRecursos.append(recursos)\n","\n","#   arrayDetecs = [detec1, detec2, detec3, detec4]\n","#   arrayDataML.append(varML)\n","#   arrayDataDetec.append(arrayDetecs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SkR5CIPNv8_"},"outputs":[],"source":["#se houver 2 arrays com a maior acurácia, é selecionado aquele que tem menos recursos.\n","maior_valor = max(arrayAcuracias)\n","if arrayAcuracias.count(maior_valor) >= 2:\n","    bestFeatures = menorArray(arrayRecursos)\n","else:\n","  posicao_maior_numero = arrayAcuracias.index(maior_valor)\n","  bestFeatures = arrayRecursos[posicao_maior_numero]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUXfoRUxQr5y"},"outputs":[],"source":["#adicionando a variável ataque para o ML\n","bestFeaturesAtaque = bestFeatures\n","bestFeaturesAtaque.append('ataque')\n","\n","#aplicando o array com os melhores recursos nos dados de ML e detecção\n","for i in range(4):\n","  df_to_update = arrayDataDetec[i]\n","  for j in range(4):\n","    df_to_update[j] = df_to_update[j].loc[:, bestFeaturesAtaque]\n","\n","for i in range(4):\n","  arrayDataML[i] = arrayDataML[i].loc[:, bestFeaturesAtaque]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGb1Oz6ZVJIN"},"outputs":[],"source":["#COMEÇANDO O APRENDIZADO FEDERADO\n","def criarDataset(dataFrame):\n","\n","  if 'ataque' in dataFrame.columns:\n","    target = dataFrame.pop('ataque')\n","    dataset = tf.data.Dataset.from_tensor_slices({'features': dataFrame.values, 'targets': target.values})\n","  else:\n","    numpy_array = np.array(dataFrame.values)\n","    dataset = tf.data.Dataset.from_tensor_slices(numpy_array)\n","\n","  return dataset\n","\n","def preprocess(dataset, batch_size, num_features):\n","\n","    # Função de formatação em lote\n","    def batch_format_fn(element):\n","\n","        return (\n","            tf.cast(tf.reshape(element['features'], [-1, num_features]), tf.float64),\n","            tf.cast(tf.reshape(element['targets'], [-1, 1]), tf.int64)\n","            )\n","\n","    # Aplicando a função de formatação em lote\n","\n","    return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzTd5bsd-vHZ"},"outputs":[],"source":["datasetsTrain = []\n","datasetTupla = []\n","\n","for i in range(4):\n","\n","  newDataSetTrain = criarDataset(arrayDataML[i])\n","\n","\n","\n","  datasetsTrain.append(newDataSetTrain)\n","\n","\n","#criando array que vai armazenar os Datasets\n","#pegando o número de features para a função preprocess\n","\n","preProcessedDataSets = []\n","num_features = arrayDataML[1].shape[1]\n","BATCH_SIZE = num_features * 5\n","\n","for i in range(4):\n","\n","  dataPreProcessed = preprocess(datasetsTrain[i], BATCH_SIZE, num_features)\n","\n","  preProcessedDataSets.append(dataPreProcessed)\n","\n","  tuplaClient = [dataPreProcessed]\n","\n","  datasetTupla.append(tuplaClient)\n","\n","\n","#finalizado a seleção de atributos e a separação de dados. tempoSF irá conter o tempo que durou o SF.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnrKVvwtKiOJ"},"outputs":[],"source":["# #VISUALIZANDO A QUANTIDADE DE DADOS TOTAL, QUANTIDADE DE DADOS PARA SF, ML, DETECÇÃO E TEMPO NECESSÁRIO\n","# num_elementos = 0\n","# print(\"quantidade de registros para SF: \" + str(len(varSF.index)))\n","\n","# print(\"#############################################\")\n","\n","# for i in range(len(arrayDataDetec)):\n","#   print(f\"quantidade de registros para detecção no node{i+1}: \")\n","#   for j in range(len(arrayDataDetec)):\n","#     print(str(len(arrayDataDetec[i][j].index)))\n","\n","# print(\"#############################################\")\n","\n","# for j in range(len(arrayDataML)):\n","#   print(f\"Quantidade de registros para ML no node{j+1}: \" +str(len(arrayDataML[j].index)))\n","\n","# print(\"#############################################\")\n","\n","# print(f\"Tempo necessário para SF e divisão dos dados: {tempoSF}\")\n","\n","# print(\"#############################################\")\n","\n","# for e in range(len(preProcessedDataSets)):\n","#   numbers = preProcessedDataSets[e].reduce(0, lambda x, _: x + 1).numpy()\n","#   num_elementos += numbers\n","# print(f\"número de dados para ML: {num_elementos}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7AzEhy2y22S"},"outputs":[],"source":["\n","element_spec = preProcessedDataSets[0].element_spec\n","\n","def create_keras_model():\n","\n","  return tf.keras.models.Sequential([\n","      tf.keras.layers.InputLayer(input_shape=(num_features,)),\n","      tf.keras.layers.Dense(40, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform()),\n","      tf.keras.layers.Dense(50, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform()),\n","      tf.keras.layers.Dense(units=1, activation='sigmoid', kernel_initializer=tf.keras.initializers.HeUniform())\n","  ])\n","\n","def model_fn():\n","\n","  keras_model = create_keras_model()\n","  return tff.learning.models.from_keras_model(\n","      keras_model,\n","      input_spec=element_spec,\n","      loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),\n","      metrics=[tf.keras.metrics.BinaryAccuracy()]\n","  )"]},{"cell_type":"code","source":["mean = tff.aggregators.MeanFactory()\n","learningprocess = tff.learning.algorithms.build_weighted_fed_avg(\n","    model_fn,\n","    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),\n","    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1, momentum=0.9),\n","    model_aggregator = mean\n",")"],"metadata":{"id":"0S7Iym0Awfne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelosClientes = []\n","for i in range(4):\n","  server_state_client = learningprocess.initialize()\n","  start_time = time.time()\n","  server_state_client, metrics = learningprocess.next(server_state_client, datasetTupla[i])\n","  print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","  #visualizar apenas a acurácia\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","  print(f'Acurácia do cliente {i + 1}:', formatted_accuracy)\n","\n","  end_time = time.time()\n","\n","  total_time = end_time - start_time\n","  print(total_time)\n","\n","  modelosClientes.append(server_state_client)\n","\n","#round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.84064984), ('loss', 0.117992006), ('num_examples', 46903), ('num_batches', 1564)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])"],"metadata":{"id":"qh81fghbIUQX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278264724,"user_tz":180,"elapsed":10355,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"1a6fe193-20d0-47ed-aa39-d72f74c53671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92954874), ('loss', 0.07030925), ('num_examples', 87706), ('num_batches', 2924)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 92.95\n","2.7564589977264404\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92770165), ('loss', 0.06951543), ('num_examples', 87706), ('num_batches', 2924)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 92.77\n","1.6412696838378906\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92666405), ('loss', 0.07116177), ('num_examples', 87706), ('num_batches', 2924)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 92.67\n","1.661238431930542\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.40253803), ('loss', 0.12321638), ('num_examples', 87706), ('num_batches', 2924)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 40.25\n","2.0258307456970215\n"]}]},{"cell_type":"code","source":["server_state = learningprocess.initialize()\n","start_time = time.time()\n","server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","print('Binary Accuracy:', formatted_accuracy)\n","\n","end_time = time.time()\n","\n","total_time = end_time - start_time\n","print(total_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZIi3OMhQVzp","executionInfo":{"status":"ok","timestamp":1687278273566,"user_tz":180,"elapsed":8855,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"12868dc8-66ce-41b3-8d5b-c7f9112f4f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.7966131), ('loss', 0.08355071), ('num_examples', 350824), ('num_batches', 11696)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Binary Accuracy: 79.66\n","8.867701292037964\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DI6Kd_lALoPX"},"outputs":[],"source":["#função para testar o modelo\n","def predict_on_new_data(server_state, new_data):\n","\n","  keras_model = create_keras_model()\n","\n","  tf.nest.map_structure(\n","      lambda var, t: var.assign(t),\n","      keras_model.trainable_weights, server_state.global_model_weights.trainable)\n","  metric = tf.keras.metrics.BinaryAccuracy()\n","\n","\n","  predictions = keras_model.predict(new_data)\n","  return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ePtMvyNx1MJ"},"outputs":[],"source":["def preprocessDATADETEC(dataset, batch_size, num_features):\n","\n","    # Função de formatação em lote\n","    def batch_format_fn(element):\n","\n","        return tf.reshape(element, [-1, num_features])\n","\n","    # Aplicando a função de formatação em lote\n","    return dataset.batch(batch_size).map(batch_format_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7osOdLXAB6XB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278462854,"user_tz":180,"elapsed":189302,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"db5f1e0b-4d74-4716-d705-e0677b0721b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["585/585 [==============================] - 1s 981us/step\n","585/585 [==============================] - 1s 2ms/step\n","585/585 [==============================] - 1s 1ms/step\n","585/585 [==============================] - 1s 941us/step\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92931867), ('loss', 0.07066287), ('num_examples', 105247), ('num_batches', 3509)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 92.93\n","2.6784286499023438\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9289291), ('loss', 0.07092404), ('num_examples', 105247), ('num_batches', 3509)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 92.89\n","2.6767640113830566\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.40039146), ('loss', 0.12366122), ('num_examples', 105247), ('num_batches', 3509)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 40.04\n","2.9205381870269775\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.39832014), ('loss', 0.12391109), ('num_examples', 105247), ('num_batches', 3509)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 1: 39.83\n","1.936152458190918\n","round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9292616), ('loss', 0.07070066), ('num_examples', 420988), ('num_batches', 14036)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 92.93\n","A quantidade de dados usado nessa rodada foi de: 14036\n","tempo da rodada 1  7.712543487548828\n","585/585 [==============================] - 1s 946us/step\n","585/585 [==============================] - 1s 1ms/step\n","585/585 [==============================] - 1s 938us/step\n","585/585 [==============================] - 1s 1ms/step\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92929274), ('loss', 0.0706809), ('num_examples', 122788), ('num_batches', 4094)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 92.93\n","2.1835520267486572\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92907286), ('loss', 0.07082763), ('num_examples', 122788), ('num_batches', 4094)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 92.91\n","2.2903997898101807\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9288937), ('loss', 0.07094598), ('num_examples', 122788), ('num_batches', 4094)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 92.89\n","2.1939704418182373\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.39816594), ('loss', 0.12394116), ('num_examples', 122788), ('num_batches', 4094)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 2: 39.82\n","2.290494441986084\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9291788), ('loss', 0.070757985), ('num_examples', 491152), ('num_batches', 16376)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 92.92\n","A quantidade de dados usado nessa rodada foi de: 16376\n","tempo da rodada 2  13.852750539779663\n","585/585 [==============================] - 1s 930us/step\n","585/585 [==============================] - 1s 971us/step\n","585/585 [==============================] - 1s 1ms/step\n","585/585 [==============================] - 1s 1ms/step\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9288814), ('loss', 0.07096156), ('num_examples', 140329), ('num_batches', 4679)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 92.89\n","3.2122297286987305\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92913085), ('loss', 0.07079003), ('num_examples', 140329), ('num_batches', 4679)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 92.91\n","3.66522479057312\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9294373), ('loss', 0.07057937), ('num_examples', 140329), ('num_batches', 4679)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 92.94\n","3.6245174407958984\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.39852062), ('loss', 0.123894855), ('num_examples', 140329), ('num_batches', 4679)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 3: 39.85\n","2.534090280532837\n","round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9292199), ('loss', 0.07073085), ('num_examples', 561316), ('num_batches', 18716)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 92.92\n","A quantidade de dados usado nessa rodada foi de: 18716\n","tempo da rodada 3  11.018933057785034\n","585/585 [==============================] - 1s 1ms/step\n","585/585 [==============================] - 1s 1ms/step\n","585/585 [==============================] - 1s 943us/step\n","585/585 [==============================] - 1s 924us/step\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92920756), ('loss', 0.07074089), ('num_examples', 157870), ('num_batches', 5264)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 92.92\n","2.7465693950653076\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9294356), ('loss', 0.070583805), ('num_examples', 157870), ('num_batches', 5264)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 92.94\n","2.82535457611084\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.9292393), ('loss', 0.070715435), ('num_examples', 157870), ('num_batches', 5264)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 92.92\n","2.7043373584747314\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.39823905), ('loss', 0.12393398), ('num_examples', 157870), ('num_batches', 5264)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do cliente 4: 39.82\n","3.6210389137268066\n","round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065253), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","Acurácia do Modelo Geral: 92.93\n","A quantidade de dados usado nessa rodada foi de: 21056\n","tempo da rodada 4  16.090123414993286\n"]}],"source":["#simulando a detecção\n","qtDataDatasets = 0\n","for i in range(4):\n","  start_time = time.time()\n","  for j in range(4):\n","\n","    #pegando o dataframe correto e transformando-o em um dataset sem rótulos\n","    data = arrayDataDetec[j][i]\n","    varAtaque = data.pop('ataque')\n","    dataset = criarDataset(data)\n","\n","    #usando o modelo global para fazer previsões\n","    preprocessedDataSet = preprocessDATADETEC(dataset, BATCH_SIZE, num_features)\n","    returned = predict_on_new_data(server_state, preprocessedDataSet)\n","\n","    #transformando o dataset em um array numpy\n","    numpyDataSet = np.array(list(dataset.as_numpy_iterator()))\n","\n","    #adicionando os valores que o modelo previu com os dados que ele utilizou\n","    result = np.concatenate((numpyDataSet, returned), axis=1)\n","\n","    #transformando o array numpy em um dataframe\n","    df = pd.DataFrame(result, columns=[bestFeatures])\n","\n","    #adicionando os valores reais para comparação\n","    df['valoresReais'] = varAtaque\n","\n","    #SUPOSIÇÃO DE QUE A PARTIR DAQUI, O TÉCNICO JÁ ARRUMOU O QUE TINHA QUE ARRUMAR E RETORNOU O DATASET\n","    #CÓDIGO QUE SIMULA O TRABALHO DO TÉCNICO\n","    df = df.drop('ataque', axis=1)\n","\n","    df = df.rename(columns={'valoresReais': 'ataque'})\n","\n","    df['ataque'] = df['ataque'].astype(int)\n","\n","    #######################\n","    #gerando um novo dataset com os dados corretos para treinar o modelo novamente\n","    dataset = criarDataset(df)\n","\n","    dataSetPreProcess = preprocess(dataset, BATCH_SIZE, num_features)\n","\n","\n","    #convertendo em um dataset\n","    preProcessedDataSets[i] = tf.data.Dataset.concatenate(preProcessedDataSets[i], dataSetPreProcess)\n","    datasetTupla[j][0] = tf.data.Dataset.concatenate(datasetTupla[j][0], dataSetPreProcess)\n","\n","#treinando e atualizando os pesos dos clientes\n","  for round in range(4):\n","    nm_preProcessedDataSets = preProcessedDataSets[round].reduce(0, lambda x, _: x + 1).numpy()\n","    qtDataDatasets += nm_preProcessedDataSets\n","\n","  #treinando os clientes individualmente\n","  for g in range(4):\n","    start_time = time.time()\n","    modelosClientes[g], metrics = learningprocess.next(modelosClientes[g], datasetTupla[g])\n","    print('round {:2d}, metrics={}'.format(1, metrics))\n","\n","    #visualizar apenas a acurácia\n","    binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","    formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","    print(f'Acurácia do cliente {i + 1}:', formatted_accuracy)\n","\n","    end_time = time.time()\n","\n","    total_time = end_time - start_time\n","    print(total_time)\n","\n","  server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","  print('round {:2d}, metrics={}'.format(i, metrics))\n","\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  formatted_accuracy = '{:.2f}'.format(binary_accuracy * 100)\n","  print('Acurácia do Modelo Geral:', formatted_accuracy)\n","  #testanto o modelo\n","  print(f\"A quantidade de dados usado nessa rodada foi de: {qtDataDatasets}\")\n","  end_time = time.time()\n","  tempoDetec = end_time - start_time\n","  print(f\"tempo da rodada {i + 1}  {tempoDetec}\")\n","  qtDataDatasets = 0\n"]},{"cell_type":"code","source":["server_state = learningprocess.initialize()\n","\n","start_time = time.time()\n","\n","bestAccuracy = 0\n","best_server_state = 0\n","\n","for i in range(16):\n","  server_state, metrics = learningprocess.next(server_state, preProcessedDataSets)\n","  binary_accuracy = metrics['client_work']['train']['binary_accuracy']\n","  print('round {:2d}, metrics={}'.format(i, metrics))\n","\n","  if binary_accuracy > bestAccuracy:\n","    bestAccuracy = binary_accuracy\n","    best_server_state = server_state\n","  elif i > 5 and binary_accuracy < bestAccuracy:\n","    print(f\"Rodada em que o modelo convergiu: {i}\")\n","    break\n","\n","end_time = time.time()\n","\n","total_time = end_time - start_time\n","print(total_time)"],"metadata":{"id":"6RFFAGdMns5q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278650013,"user_tz":180,"elapsed":187162,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"83c21274-0dad-4386-8fd2-281bcfa6aac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.796391), ('loss', 0.08376449), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065092), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.070651785), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065223), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065252), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065274), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.070652895), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065302), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065313), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065321), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 10, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065328), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 11, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065333), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 12, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065338), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 13, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.070653416), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 14, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.07065345), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","round 15, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.92933583), ('loss', 0.070653476), ('num_examples', 631480), ('num_batches', 21056)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n","187.11908173561096\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"5LL2YyABDNXm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pY5r2cKBnGu1"},"outputs":[],"source":["\n","# start_time = time.time()\n","# for round in range(20):\n","#     server_state = federated_algorithm.next(server_state, preProcessedDataSets)\n","\n","# #testanto o modelo\n","# evaluate(server_state)\n","\n","# end_time = time.time()\n","# tempoExtra = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Od_uc54STs-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278650014,"user_tz":180,"elapsed":11,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"4bb3be44-964a-4caf-8b6d-06ad703477bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["18.447888374328613\n"]}],"source":["print(tempoSF)\n","# print(tempoDetec)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcAnkL2nSuIF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687278650014,"user_tz":180,"elapsed":9,"user":{"displayName":"Douglas Ferreira","userId":"11002289108936304578"}},"outputId":"7faa030c-83e8-4311-d27a-4efe91f3989d"},"outputs":[{"output_type":"stream","name":"stdout","text":["O número total de registros é 701648.\n","Dados para ML 350824\n","30\n"]}],"source":["# Contando o número de registros em todos os dataframes\n","num_registros = sum(df.shape[0] for df in nodes)\n","\n","# Exibindo o número total de registros\n","print(f\"O número total de registros é {num_registros}.\")\n","\n","num_registros = sum(df.shape[0] for df in arrayDataML)\n","\n","print(f\"Dados para ML {num_registros}\")\n","\n","print(BATCH_SIZE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pks-5CCFTn0a"},"outputs":[],"source":["# print(nodes[0].shape)\n","# print(nodes[1].shape)\n","# print(nodes[2].shape)\n","# print(nodes[3].shape)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1vN_N9m4UMFF8vtBgZREX2HH6isPsRFVh","timestamp":1684517423398},{"file_id":"146EF9yzd1aG_qatqFMoNg8-KkWHHUpxT","timestamp":1684161661680}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}